DEPLOYMENT_ID = ${tf_deployment_id}
DEPLOY_REGION = ${tf_deploy_region}
EKS_CLUSTER_NAME = ${tf_eks_cluster_name}
FQDN = ${tf_fqdn}
CXONE_VERSION = ${tf_cxone_version}
RELEASE_CHANNEL = ${tf_release_channel}
KOTS_PASSWORD = ${tf_kots_password}
NAMESPACE = ${tf_namespace}
LICENSE_FILE = ${tf_license_file}
KOTS_CONFIG_FILE = ${tf_kots_config_file}
AIRGAP_BUNDLE = ???
KOTS_REGISTRY = ???
REGISTRY_USERNAME = ???
REGISTRY_PASSWORD = ???
TOTP = 123



.PHONY: update-kubeconfig
update-kubeconfig:
	aws eks update-kubeconfig --name $${EKS_CLUSTER_NAME}


.PHONY: kots-install
kots-install:
	kubectl kots install ast/$${RELEASE_CHANNEL} -n $${NAMESPACE} --license-file $${LICENSE_FILE} --shared-password '$${KOTS_PASSWORD}' --config-values $${KOTS_CONFIG_FILE} --app-version-label $${CXONE_VERSION}


.PHONY: kots-install-from-airgap
kots-install-from-airgap:
	kubectl kots install ast/$${RELEASE_CHANNEL} -n $${NAMESPACE} --license-file $${LICENSE_FILE} --shared-password '$${KOTS_PASSWORD}' --config-values $${KOTS_CONFIG_FILE} \
	--airgap --airgap-bundle $${AIRGAP_BUNDLE} --kotsadm-registry $${KOTS_REGISTRY} --registry-username $${REGISTRY_USERNAME} --registry-password '$${REGISTRY_PASSWORD}'


.PHONY: kots-set-config
kots-set-config:
	kubectl kots set config ast -n $${NAMESPACE} --config-file $${KOTS_CONFIG_FILE} --deploy --current	


.PHONY: kots-get-config
kots-get-config:
	kubectl kots get config -n $${NAMESPACE} --appslug ast


.PHONY: kots-admin-console
kots-admin-console:
	kubectl kots admin-console -n $${NAMESPACE}


.PHONY: rollout-restart
rollout-restart:
	kubectl -n $${NAMESPACE} rollout restart deploy
	kubectl -n $${NAMESPACE} rollout restart statefulset


.PHONY: install-cluster-autoscaler
install-cluster-autoscaler:
	helm repo add autoscaler https://kubernetes.github.io/autoscaler; \
	helm repo update autoscaler; \
	helm install cluster-autoscaler autoscaler/cluster-autoscaler \
	--version 9.21.1 \
	-n kube-system \
	--set awsRegion=$${DEPLOY_REGION} \
	--set rbac.serviceAccount.create=true \
	--set rbac.serviceAccount.name=cluster-autoscaler \
	--set rbac.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${cluster_autoscaler_iam_role_arn}" \
	--set autoDiscovery.clusterName=$${EKS_CLUSTER_NAME}


.PHONY: uninstall-cluster-autoscaler
uninstall-cluster-autoscaler:
	helm uninstall cluster-autoscaler -n kube-system


.PHONY: install-external-dns
install-external-dns:
	helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/; \
	helm upgrade --install external-dns external-dns/external-dns \
	-n kube-system \
	--version 1.11.0 \
	--set serviceAccount.create=true \
	--set serviceAccount.name=external-dns \
	--set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${external_dns_iam_role_arn}"


.PHONY: uninstall-external-dns
uninstall-external-dns:
	helm uninstall external-dns -n kube-system


.PHONY: install-load-balancer-controller
install-load-balancer-controller:
	helm repo add eks https://aws.github.io/eks-charts; \
	helm repo update eks; \
	helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
	--version 1.7.1 \
	-n kube-system \
	--set vpcId=${vpc_id} \
	--set region=$${DEPLOY_REGION} \
	--set serviceAccount.create=true \
	--set serviceAccount.name=aws-load-balancer-controller \
	--set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${load_balancer_controller_iam_role_arn}" \
	--set clusterName=$${EKS_CLUSTER_NAME} \
	--set enableShield=false \
	--set enableWaf=false \
	--set enableWaafv2=false


.PHONY: uninstall-load-balancer-controller
uninstall-load-balancer-controller:
	helm uninstall aws-load-balancer-controller -n kube-system


.PHONY: install-karpenter
install-karpenter:
	helm install karpenter oci://public.ecr.aws/karpenter/karpenter \
	--version 0.36.0 \
	-n kube-system \
	--create-namespace \
	--set serviceAccount.create=true \
	--set serviceAccount.name=karpenter \
	--set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${karpenter_iam_role_arn}" \
	--set settings.clusterName=$${EKS_CLUSTER_NAME} \
	--set settings.clusterEndpoint=${cluster_endpoint} \
	--set settings.featureGates.spotToSpotConsolidation=true \
	--set settings.interruptionQueue=$${EKS_CLUSTER_NAME}-node-termination-handler \
	--set controller.resources.requests.cpu=1 \
	--set controller.resources.requests.memory=1Gi \
	--set controller.resources.limits.cpu=1 \
	--set controller.resources.limits.memory=1Gi
	kubectl apply -f karpenter.$${DEPLOYMENT_ID}.yaml 


.PHONY: uninstall-karpenter
uninstall-karpenter:
	helm uninstall karpenter -n kube-system


.PHONY: view-firewall-logs
view-firewall-logs:
	aws logs filter-log-events --start-time 1713475743 --log-group-name /aws/vendedlogs/$${DEPLOYMENT_ID}-aws-nfw-alert | jq -r ' .events[].message' |  jq ' (.event.timestamp + " " + .event.alert.action + ": " + .event.src_ip + ":" + (.event.src_port|tostring) + " -> " + .event.proto + "/" + .event.app_proto + " "  + .event.dest_ip + ":" + (.event.dest_port|tostring) + " " + .event.tls.sni + .event.http.hostname) + " " + .event.http.http_user_agent + " " + .event.http.http_method + " " + .event.http.url'


.PHONY: apply-storageclass-config
apply-storageclass-config:
	./apply-storageclass-config.$${DEPLOYMENT_ID}.sh


.PHONY: clean-kots
clean-kots:
	kubectl delete deployment kotsadm -n $${NAMESPACE}
	kubectl delete statefulset kotsadm-minio -n $${NAMESPACE}
	kubectl delete statefulset kotsadm-rqlite -n $${NAMESPACE}


.PHONY: destroy-load-balancer
destroy-load-balancer:
	./destroy-load-balancer.$${DEPLOYMENT_ID}.sh


.PHONY: totp
totp:
	echo "$${TOTP}" | totp-cli instant
